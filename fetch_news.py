# -*- coding: utf-8 -*-
"""
fetch_news.py
========================================
ç”Ÿæˆ Michael News ç½‘ç«™æ•°æ®ï¼ˆdocs/news.json + docs/site_meta.jsonï¼‰

åŠŸèƒ½ï¼š
- æŠ“å– BBC World + NHK cat0 RSS
- å¯¹æ¯æ¡æ–°é—»æ‰“å¼€ç½‘é¡µï¼Œæå–â€œç¬¬ä¸€æ®µâ€ä½œä¸ºæ‘˜è¦
- ç¿»è¯‘æˆä¸­æ–‡ï¼š
  - è‹±æ–‡ï¼šen -> zh
  - æ—¥æ–‡ï¼šja -> en -> zhï¼ˆå› ä¸ºå¾ˆå¤šç¯å¢ƒä¸‹æ‰¾ä¸åˆ° ja->zh æ¨¡å‹ï¼‰
- ç»“æœå†™å…¥ docs/news.jsonï¼ˆä¾› GitHub Pages é™æ€ç½‘é¡µè¯»å–ï¼‰

ç”¨æ³•ï¼ˆGitHub Actions æ¨èï¼‰ï¼š
- å®‰è£…ç¿»è¯‘æ¨¡å‹ï¼ˆå¯å¤±è´¥ï¼Œä¸å½±å“åç»­ç”Ÿæˆï¼‰ï¼š
    python fetch_news.py --install-models
- ç”Ÿæˆç½‘ç«™æ•°æ®ï¼ˆå…¨é‡ï¼‰ï¼š
    python fetch_news.py --all

æœ¬åœ°è°ƒè¯•ï¼š
- åªçœ‹æ–°å¢ï¼š
    python fetch_news.py --new --limit 5
"""

import argparse
import hashlib
import json
import os
import re
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import feedparser
import requests
from bs4 import BeautifulSoup
from dateutil import parser as date_parser

# =========================
# 1) å¯é…ç½®é¡¹
# =========================

RSS_FEEDS = [
    {
        "name": "BBC News",
        "url": "http://newsrss.bbc.co.uk/rss/newsonline_uk_edition/world/rss.xml",
    },
    {
        "name": "NHKãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "url": "https://www3.nhk.or.jp/rss/news/cat0.xml",
    },
]

REQUEST_TIMEOUT_SECONDS = 12
REQUEST_RETRY_TIMES = 2
REQUEST_RETRY_SLEEP_SECONDS = 1

ARTICLE_FETCH_SLEEP_SECONDS = 0.25

DEFAULT_PRINT_LIMIT = 20

SEEN_FILE = "seen.json"
TRANSLATION_CACHE_FILE = "translation_cache.json"

DOCS_DIR = "docs"
NEWS_JSON_PATH = os.path.join(DOCS_DIR, "news.json")
SITE_META_PATH = os.path.join(DOCS_DIR, "site_meta.json")

# =========================
# 2) å°å·¥å…·
# =========================

def print_cn(msg: str) -> None:
    print(msg)

def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)

def safe_get_str(value, default: str = "") -> str:
    if value is None:
        return default
    try:
        s = str(value).strip()
        return s if s else default
    except Exception:
        return default

def load_json(path: str, default):
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def save_json(path: str, data) -> None:
    ensure_dir(os.path.dirname(path) or ".")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def sha1_text(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def looks_japanese(text: str) -> bool:
    # ç²—ç•¥åˆ¤æ–­ï¼šå‡ºç°å‡å/å¸¸ç”¨æ—¥æ–‡å­—ç¬¦å°±å½“æ—¥æ–‡
    return bool(re.search(r"[\u3040-\u30ff\u3400-\u4dbf\u4e00-\u9fff]", text or ""))

def parse_datetime_from_entry(entry: dict) -> Optional[datetime]:
    for key in ("published_parsed", "updated_parsed"):
        parsed = entry.get(key)
        if parsed:
            try:
                ts = time.mktime(parsed)
                return datetime.fromtimestamp(ts).astimezone()
            except Exception:
                pass

    for key in ("published", "updated"):
        text = entry.get(key)
        if text:
            try:
                dt = date_parser.parse(str(text))
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=datetime.now().astimezone().tzinfo)
                return dt.astimezone()
            except Exception:
                pass

    return None

def requests_get_with_retry(url: str) -> Optional[requests.Response]:
    headers = {"User-Agent": "michael-news-bot/1.0"}
    attempt_total = REQUEST_RETRY_TIMES + 1

    for attempt in range(1, attempt_total + 1):
        try:
            resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT_SECONDS)
            resp.raise_for_status()
            return resp
        except requests.exceptions.RequestException as e:
            if attempt < attempt_total:
                print_cn(f"âš ï¸ æŠ“å–å¤±è´¥ï¼ˆç¬¬ {attempt}/{attempt_total} æ¬¡ï¼‰ï¼š{e}")
                print_cn(f"   {REQUEST_RETRY_SLEEP_SECONDS} ç§’åé‡è¯•...")
                time.sleep(REQUEST_RETRY_SLEEP_SECONDS)
            else:
                print_cn(f"âŒ æŠ“å–å¤±è´¥ï¼ˆå·²é‡è¯• {REQUEST_RETRY_TIMES} æ¬¡ä»å¤±è´¥ï¼‰ï¼š{e}")
                return None
    return None

def build_item_key(title: str, link: str) -> str:
    return link if link else title

# =========================
# 3) æŠ“æ–‡ç« â€œç¬¬ä¸€æ®µâ€
# =========================

def extract_first_paragraph(url: str, html: str) -> str:
    """
    ä»æ–‡ç« é¡µ HTML æå–â€œç¬¬ä¸€æ®µâ€æ­£æ–‡ã€‚
    ä¼˜å…ˆç«™ç‚¹è§„åˆ™ï¼Œå…¶æ¬¡é€šç”¨è§„åˆ™ã€‚
    """
    soup = BeautifulSoup(html, "html.parser")

    # å»æ‰æ— ç”¨å†…å®¹
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    host = urlparse(url).netloc.lower()

    def first_good_paragraph(container) -> str:
        if not container:
            return ""
        ps = container.find_all("p")
        for p in ps:
            t = normalize_text(p.get_text(" ", strip=True))
            # è¿‡æ»¤å¤ªçŸ­/å¯¼èˆªç±»
            if len(t) >= 30:
                return t
        # å…œåº•ï¼šå–ç¬¬ä¸€ä¸ªéç©º
        for p in ps:
            t = normalize_text(p.get_text(" ", strip=True))
            if t:
                return t
        return ""

    # ---- NHK ----
    if "nhk.or.jp" in host:
        candidates = [
            soup.select_one("#js-article-body"),
            soup.select_one(".content--detail-body"),
            soup.select_one("article"),
            soup.select_one("main"),
        ]
        for c in candidates:
            t = first_good_paragraph(c)
            if t:
                return t

    # ---- BBC ----
    if "bbc." in host:
        container = soup.select_one("main") or soup.select_one("article")
        t = first_good_paragraph(container)
        if t:
            return t

    # ---- é€šç”¨ ----
    container = soup.select_one("article") or soup.select_one("main")
    t = first_good_paragraph(container)
    if t:
        return t

    # ---- æœ€åå…œåº•ï¼šå…¨ç«™ p ----
    for p in soup.find_all("p"):
        t = normalize_text(p.get_text(" ", strip=True))
        if len(t) >= 30:
            return t
    for p in soup.find_all("p"):
        t = normalize_text(p.get_text(" ", strip=True))
        if t:
            return t
    return ""

def fetch_first_paragraph(url: str) -> str:
    if not url:
        return ""
    resp = requests_get_with_retry(url)
    if resp is None:
        return ""
    html = resp.text
    return extract_first_paragraph(url, html)

# =========================
# 4) ç¦»çº¿ç¿»è¯‘ï¼ˆArgosï¼‰
# =========================

def _try_import_argos():
    try:
        import argostranslate.package  # type: ignore
        import argostranslate.translate  # type: ignore
        return True
    except Exception:
        return False

ARGOS_AVAILABLE = _try_import_argos()

def load_translation_cache() -> Dict[str, str]:
    data = load_json(TRANSLATION_CACHE_FILE, default={})
    return data if isinstance(data, dict) else {}

def save_translation_cache(cache: Dict[str, str]) -> None:
    save_json(TRANSLATION_CACHE_FILE, cache)

def argos_installed_languages() -> Set[Tuple[str, str]]:
    """
    è¿”å›å·²å®‰è£…è¯­è¨€å¯¹ (from_code, to_code)
    """
    if not ARGOS_AVAILABLE:
        return set()
    import argostranslate.translate  # type: ignore
    langs = argostranslate.translate.get_installed_languages()
    pairs = set()
    for l in langs:
        for t in l.translations:
            pairs.add((l.code, t.to_lang.code))
    return pairs

def argos_translate(text: str, from_code: str, to_code: str) -> Optional[str]:
    if not ARGOS_AVAILABLE:
        return None
    import argostranslate.translate  # type: ignore

    installed = argos_installed_languages()
    if (from_code, to_code) not in installed:
        return None

    try:
        return argostranslate.translate.translate(text, from_code, to_code)
    except Exception:
        return None

def translate_to_zh(text: str, prefer_lang: str) -> str:
    """
    prefer_lang: 'en' or 'ja' (æ¥æºè¯­è¨€çš„åå¥½)
    ç¿»è¯‘é€»è¾‘ï¼š
    - å¦‚æœæ¥æºæ˜¯è‹±æ–‡ï¼šen->zh
    - å¦‚æœæ¥æºæ˜¯æ—¥æ–‡ï¼š
        1) å°è¯• ja->zhï¼ˆå¦‚æœæœ‰ï¼‰
        2) å¦åˆ™ ja->en å† en->zhï¼ˆæ¨èè·¯å¾„ï¼‰
    """
    text = safe_get_str(text, "")
    if not text:
        return ""

    if not ARGOS_AVAILABLE:
        return ""

    cache = translate_to_zh._cache  # type: ignore
    key = sha1_text(f"{prefer_lang}||{text}")
    if key in cache:
        return cache[key]

    result = ""

    if prefer_lang == "en":
        r = argos_translate(text, "en", "zh")
        result = r or ""
    else:
        # ja source
        direct = argos_translate(text, "ja", "zh")
        if direct:
            result = direct
        else:
            mid = argos_translate(text, "ja", "en")
            if mid:
                final = argos_translate(mid, "en", "zh")
                result = final or ""

    cache[key] = result
    return result

translate_to_zh._cache = load_translation_cache()  # type: ignore

def install_argos_models() -> int:
    """
    å®‰è£…éœ€è¦çš„ Argos æ¨¡å‹ï¼š
    - en -> zh
    - ja -> en
    - (å¯é€‰) ja -> zhï¼ˆå¤šæ•°æ—¶å€™ç´¢å¼•é‡Œæ²¡æœ‰ï¼Œä¸å¼ºæ±‚ï¼‰

    è¿”å›ï¼š
    - 0ï¼šæ‰§è¡Œå®Œæˆï¼ˆå³ä½¿ç¼º ja->zh ä¹Ÿç®—æˆåŠŸï¼‰
    - 1ï¼šæ›´æ–°ç´¢å¼•/ä¸‹è½½ä¸¥é‡å¤±è´¥
    """
    if not ARGOS_AVAILABLE:
        print_cn("âŒ æœªå®‰è£… argostranslateï¼Œè·³è¿‡æ¨¡å‹å®‰è£…ã€‚")
        print_cn("   è§£å†³ï¼špython -m pip install argostranslate")
        return 1

    import argostranslate.package  # type: ignore

    def retry(fn, times=3, sleep_s=2):
        last_err = None
        for i in range(times):
            try:
                return fn()
            except Exception as e:
                last_err = e
                print_cn(f"âš ï¸ æ¨¡å‹ç´¢å¼•/ä¸‹è½½å¤±è´¥ï¼ˆç¬¬ {i+1}/{times} æ¬¡ï¼‰ï¼š{e}")
                time.sleep(sleep_s)
        raise last_err  # type: ignore

    print_cn("ğŸŒ æ­£åœ¨æ›´æ–° Argos æ¨¡å‹ç´¢å¼•ï¼ˆéœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹ï¼‰...")

    try:
        retry(argostranslate.package.update_package_index, times=3, sleep_s=2)
        available_packages = argostranslate.package.get_available_packages()
    except Exception as e:
        print_cn(f"âŒ æ›´æ–°æ¨¡å‹ç´¢å¼•å¤±è´¥ï¼š{e}")
        return 1

    def find_pkg(frm: str, to: str):
        for p in available_packages:
            if p.from_code == frm and p.to_code == to:
                return p
        return None

    wanted = [("en", "zh"), ("ja", "en"), ("ja", "zh")]

    for frm, to in wanted:
        pkg = find_pkg(frm, to)
        if not pkg:
            print_cn(f"âš ï¸ æœªåœ¨ç´¢å¼•ä¸­æ‰¾åˆ°ï¼š{frm}->{to}")
            continue
        try:
            print_cn(f"â¬‡ï¸ å‘ç°æ¨¡å‹ {frm}->{to}ï¼Œå¼€å§‹ä¸‹è½½å¹¶å®‰è£…...")
            download_path = pkg.download()
            argostranslate.package.install_from_path(download_path)
            print_cn(f"âœ… å·²å®‰è£…ï¼š{frm}->{to}")
        except Exception as e:
            print_cn(f"âš ï¸ å®‰è£…å¤±è´¥ {frm}->{to}ï¼š{e}")

    print_cn("âœ… æ¨¡å‹å®‰è£…æµç¨‹ç»“æŸï¼ˆå³ä½¿ç¼º ja->zh ä¹Ÿæ²¡å…³ç³»ï¼Œæ—¥æ–‡ä¼šèµ° ja->en->zhï¼‰ã€‚")
    return 0

# =========================
# 5) RSS æŠ“å–/åˆå¹¶/å¢é‡
# =========================

def load_seen(file_path: str) -> Set[str]:
    data = load_json(file_path, default={"seen": []})
    seen_list = data.get("seen", []) if isinstance(data, dict) else []
    if not isinstance(seen_list, list):
        return set()
    return set(str(x) for x in seen_list)

def save_seen(file_path: str, seen_set: Set[str]) -> None:
    save_json(file_path, {"seen": sorted(seen_set)})

def fetch_and_parse_one_feed(feed_name: str, feed_url: str) -> List[Dict]:
    print_cn(f"ğŸ“° æ­£åœ¨æŠ“å– {feed_name}ï¼š{feed_url}")

    resp = requests_get_with_retry(feed_url)
    if resp is None:
        print_cn(f"âŒ è·³è¿‡ {feed_name}ï¼ˆæŠ“å–å¤±è´¥ï¼‰")
        return []

    parsed = feedparser.parse(resp.content)

    feed_title = safe_get_str(parsed.get("feed", {}).get("title"), default=feed_name)
    source_name = feed_title if feed_title else feed_name

    entries = parsed.get("entries", [])
    print_cn(f"âœ… {feed_name} æŠ“å–æˆåŠŸï¼Œè§£æåˆ° {len(entries)} æ¡æ¡ç›®")

    now_dt = datetime.now().astimezone()
    items: List[Dict] = []

    for entry in entries:
        title = safe_get_str(entry.get("title"), default="(æ— æ ‡é¢˜)")
        link = safe_get_str(entry.get("link"), default="")

        dt = parse_datetime_from_entry(entry) or now_dt
        published_str = dt.strftime("%Y-%m-%d %H:%M:%S%z")
        if len(published_str) >= 5:
            published_str = published_str[:-5] + published_str[-5:-2] + ":" + published_str[-2:]

        item_key = build_item_key(title=title, link=link)

        items.append({
            "source": source_name,
            "published": published_str,
            "_published_ts": dt.timestamp(),
            "title": title,
            "link": link,
            "_key": item_key,
        })

    return items

def merge_sort_dedupe(items: List[Dict]) -> List[Dict]:
    items_sorted = sorted(items, key=lambda x: x.get("_published_ts", 0), reverse=True)
    seen_in_run: Set[str] = set()
    unique_items: List[Dict] = []

    for it in items_sorted:
        key = safe_get_str(it.get("_key"), default="")
        if not key:
            key = f"__empty__{it.get('_published_ts', 0)}"
        if key in seen_in_run:
            continue
        seen_in_run.add(key)
        unique_items.append(it)

    return unique_items

def filter_new_items(items: List[Dict], seen_before: Set[str], mode_new: bool) -> Tuple[List[Dict], Set[str]]:
    updated_seen = set(seen_before)

    if not mode_new:
        for it in items:
            k = safe_get_str(it.get("_key"), default="")
            if k:
                updated_seen.add(k)
        return items, updated_seen

    new_items: List[Dict] = []
    for it in items:
        k = safe_get_str(it.get("_key"), default="")
        if not k:
            continue
        if k not in seen_before:
            new_items.append(it)
        updated_seen.add(k)

    return new_items, updated_seen

# =========================
# 6) æ„å»ºç½‘ç«™æ•°æ®
# =========================

def build_output_items(selected_items: List[Dict]) -> List[Dict]:
    """
    ç”Ÿæˆæœ€ç»ˆå†™å…¥ docs/news.json çš„ç»“æ„ï¼š
    - title_orig / title_zh
    - summary_orig / summary_zhï¼ˆç¬¬ä¸€æ®µï¼‰
    """
    out: List[Dict] = []

    need_translate = ARGOS_AVAILABLE and len(argos_installed_languages()) > 0

    if selected_items:
        print_cn(f"ğŸ§¾ æ­£åœ¨ä¸ºæœ¬æ¬¡è¾“å‡ºçš„ {len(selected_items)} æ¡æ–°é—»æŠ“å–â€œç¬¬ä¸€æ®µæ‘˜è¦â€...")
    for i, it in enumerate(selected_items, start=1):
        link = safe_get_str(it.get("link"), "")
        title = safe_get_str(it.get("title"), "")
        source = safe_get_str(it.get("source"), "")
        published = safe_get_str(it.get("published"), "")

        summary = ""
        if link:
            print_cn(f"   [{i}/{len(selected_items)}] æŠ“æ‘˜è¦ï¼š{link}")
            summary = fetch_first_paragraph(link)
            time.sleep(ARTICLE_FETCH_SLEEP_SECONDS)

        # è¯­è¨€åˆ¤å®šï¼ˆä¼˜å…ˆç”¨æ¥æºï¼Œå…¶æ¬¡çœ‹æ–‡æœ¬ï¼‰
        is_nhk = "nhk" in source.lower()
        prefer_lang = "ja" if (is_nhk or looks_japanese(title + " " + summary)) else "en"

        title_zh = ""
        summary_zh = ""
        if need_translate:
            title_zh = translate_to_zh(title, prefer_lang=prefer_lang)
            summary_zh = translate_to_zh(summary, prefer_lang=prefer_lang)

        out.append({
            "source": source,
            "published": published,
            "link": link,
            "title_orig": title,
            "title_zh": title_zh,
            "summary_orig": summary,
            "summary_zh": summary_zh,
        })

    # ä¿å­˜ç¿»è¯‘ç¼“å­˜ï¼ˆå¾ˆé‡è¦ï¼šåŠ é€Ÿ + å‡å°‘é‡å¤ç¿»è¯‘ï¼‰
    save_translation_cache(translate_to_zh._cache)  # type: ignore
    return out

def write_site(news_items: List[Dict]) -> None:
    ensure_dir(DOCS_DIR)
    save_json(NEWS_JSON_PATH, news_items)

    meta = {
        "site_title": "Michael News",
        "last_updated": datetime.now().astimezone().strftime("%Y-%m-%d %H:%M:%S %z"),
        "count": len(news_items),
    }
    save_json(SITE_META_PATH, meta)

def print_items(items: List[Dict], limit: int) -> None:
    if not items:
        print_cn("ï¼ˆæœ¬æ¬¡æ²¡æœ‰éœ€è¦è¾“å‡ºçš„æ–°é—»ï¼‰")
        return

    print_cn("")
    print_cn(f"ğŸ“Œ ç»ˆç«¯å±•ç¤ºæœ€æ–° {min(limit, len(items))} æ¡ï¼š")
    print_cn("------------------------------------------------------------")
    for idx, it in enumerate(items[:limit], start=1):
        print_cn(f"{idx}. [{it.get('published', '')}] ({it.get('source', '')})")
        t0 = safe_get_str(it.get("title_orig"), "")
        tz = safe_get_str(it.get("title_zh"), "")
        s0 = safe_get_str(it.get("summary_orig"), "")
        sz = safe_get_str(it.get("summary_zh"), "")

        if tz:
            print_cn(f"   æ ‡é¢˜ï¼š{t0}ï¼ˆ{tz}ï¼‰")
        else:
            print_cn(f"   æ ‡é¢˜ï¼š{t0}ï¼ˆæœªç¿»è¯‘ï¼‰")

        print_cn(f"   é“¾æ¥ï¼š{it.get('link', '')}")

        if sz:
            print_cn(f"   æ‘˜è¦ï¼š{s0}ï¼ˆ{sz}ï¼‰")
        else:
            # å…è®¸æ‘˜è¦ä¸ºç©º
            if s0:
                print_cn(f"   æ‘˜è¦ï¼š{s0}ï¼ˆæœªç¿»è¯‘ï¼‰")
            else:
                print_cn("   æ‘˜è¦ï¼šï¼ˆæœªæå–åˆ°ç¬¬ä¸€æ®µï¼Œå¯èƒ½æ˜¯ç½‘ç«™ç»“æ„å˜åŒ–/åçˆ¬/ç½‘ç»œé—®é¢˜ï¼‰")
        print_cn("")
    print_cn("------------------------------------------------------------")

# =========================
# 7) CLI
# =========================

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="æŠ“å– RSS æ–°é—»ï¼Œç”Ÿæˆ Michael News ç«™ç‚¹æ•°æ®ï¼ˆå¸¦ä¸­æ–‡ç¿»è¯‘ï¼‰")

    group = parser.add_mutually_exclusive_group()
    group.add_argument("--new", action="store_true", help="åªè¾“å‡ºæ–°å¢ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰")
    group.add_argument("--all", action="store_true", help="è¾“å‡ºå…¨éƒ¨ï¼ˆä¸åšå¢é‡è¿‡æ»¤ï¼‰")

    parser.add_argument("--limit", type=int, default=DEFAULT_PRINT_LIMIT, help="ç»ˆç«¯æ‰“å°æ¡æ•°ï¼ˆé»˜è®¤ 20ï¼‰")
    parser.add_argument("--install-models", action="store_true", help="å®‰è£…/æ›´æ–° Argos ç¿»è¯‘æ¨¡å‹ï¼ˆéœ€è¦è”ç½‘ï¼‰")
    return parser.parse_args()

def main() -> None:
    args = parse_args()

    if args.install_models:
        code = install_argos_models()
        # ä¸å¼ºåˆ¶å¤±è´¥ï¼šè®© Actions æ›´ç¨³å®šï¼ˆå³ä½¿ç½‘ç»œæŠ½é£ä¹Ÿä¸å½±å“åç»­ç”Ÿæˆï¼‰
        sys.exit(0 if code == 0 else 0)

    mode_new = True
    if args.all:
        mode_new = False

    seen_before = load_seen(SEEN_FILE)

    all_items: List[Dict] = []
    for feed in RSS_FEEDS:
        name = safe_get_str(feed.get("name"), default="(æœªå‘½åRSS)")
        url = safe_get_str(feed.get("url"), default="")
        items = fetch_and_parse_one_feed(feed_name=name, feed_url=url)
        all_items.extend(items)

    if not all_items:
        print_cn("âš ï¸ æ²¡æœ‰æŠ“åˆ°ä»»ä½•æ¡ç›®ã€‚è¯·æ£€æŸ¥ç½‘ç»œæˆ– RSS é“¾æ¥æ˜¯å¦å¯ç”¨ã€‚")
        # ä»ç„¶å†™ä¸€ä¸ªç©ºç«™ç‚¹ï¼Œé¿å…ç½‘é¡µå´©
        write_site([])
        return

    merged_unique = merge_sort_dedupe(all_items)
    print_cn(f"ğŸ” åˆå¹¶åå»é‡ï¼š{len(merged_unique)} æ¡ï¼ˆæ¥è‡ª {len(RSS_FEEDS)} ä¸ªæºï¼‰")

    selected_items, updated_seen = filter_new_items(
        items=merged_unique,
        seen_before=seen_before,
        mode_new=mode_new,
    )

    if mode_new:
        print_cn(f"ğŸ†• æ–°å¢æ–°é—»ï¼š{len(selected_items)} æ¡ï¼ˆé»˜è®¤åªè¾“å‡ºæ–°å¢ï¼‰")
    else:
        print_cn(f"ğŸ“¦ è¾“å‡ºå…¨éƒ¨ï¼š{len(selected_items)} æ¡ï¼ˆä¸åšå¢é‡ï¼‰")

    # æ„å»ºæœ€ç»ˆè¾“å‡º
    output_items = build_output_items(selected_items if mode_new else merged_unique)

    # å†™ç«™ç‚¹æ•°æ®ï¼ˆç½‘é¡µè¯»å– docs/news.jsonï¼‰
    write_site(output_items)

    # æ›´æ–° seen
    save_seen(SEEN_FILE, updated_seen)

    # ç»ˆç«¯å±•ç¤º
    limit = max(1, int(args.limit))
    print_items(output_items, limit=limit)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print_cn("\nğŸ›‘ ä½ æ‰‹åŠ¨ä¸­æ–­äº†ç¨‹åºï¼ˆCtrl+Cï¼‰")
        sys.exit(0)
    except Exception as e:
        print_cn(f"\nâŒ ç¨‹åºå‘ç”Ÿæœªæ•è·å¼‚å¸¸ï¼š{e}")
        sys.exit(1)
