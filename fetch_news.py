#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
RSS News -> Static site data builder (GitHub Pages)

ç›®æ ‡ï¼š
- æŠ“å– RSSï¼ˆBBC World + NHK cat0ï¼‰
- ä¸ºæ¯æ¡æ–°é—»æŠ“å–â€œç¬¬ä¸€æ®µåŸæ–‡â€
- æ ‡é¢˜ & ç¬¬ä¸€æ®µéƒ½ç¿»è¯‘æˆä¸­æ–‡
  - BBC: en -> zh ç›´æ¥
  - NHK: ja -> en -> zhï¼ˆå› ä¸ºé€šå¸¸æ²¡æœ‰ ja->zh æ¨¡å‹ï¼‰
- ç”Ÿæˆ docs/data.json ç»™ GitHub Pages ç«™ç‚¹ä½¿ç”¨

ç”¨æ³•ï¼ˆæœ¬åœ°/Actionsï¼‰ï¼š
- å®‰è£…æ¨¡å‹ï¼ˆActions ç”¨ï¼‰ï¼špython fetch_news.py --install-models
- æ„å»ºç«™ç‚¹æ•°æ®ï¼špython fetch_news.py --build-site --limit 50
- ç»ˆç«¯æŸ¥çœ‹ï¼špython fetch_news.py --all --limit 3
"""

import argparse
import json
import os
import re
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple

import feedparser
import requests
from bs4 import BeautifulSoup
from dateutil import parser as dateparser

# -------------------------
# é…ç½®ï¼šRSS æº
# -------------------------
SOURCES = [
    {
        "name": "BBC News",
        "lang": "en",
        "rss": "http://newsrss.bbc.co.uk/rss/newsonline_uk_edition/world/rss.xml",
    },
    {
        "name": "NHKãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "lang": "ja",
        "rss": "https://www3.nhk.or.jp/rss/news/cat0.xml",
    },
]

UA = {
    "User-Agent": "Mozilla/5.0 (compatible; MichaelNewsBot/1.0; +https://github.com/)"
}

DEFAULT_TIMEOUT = 15
RETRY = 3
SLEEP_BETWEEN = 1

DATA_OUT_PATH = os.path.join("docs", "data.json")


# -------------------------
# å·¥å…·ï¼šè¾“å‡º
# -------------------------
def log(msg: str) -> None:
    print(msg, flush=True)


def safe_mkdir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def normalize_ws(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def truncate(s: str, n: int) -> str:
    s = s or ""
    return s if len(s) <= n else s[:n].rstrip() + "..."


def parse_dt(entry: Any) -> Optional[datetime]:
    # feedparser å¯èƒ½ç»™ published / updated / created
    for k in ("published", "updated", "created"):
        if k in entry and entry[k]:
            try:
                dt = dateparser.parse(entry[k])
                if not dt.tzinfo:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt
            except Exception:
                pass
    return None


def fmt_dt(dt: Optional[datetime]) -> str:
    if not dt:
        return ""
    # è¾“å‡º ISO + æ—¶åŒº
    return dt.astimezone().isoformat(timespec="seconds")


# -------------------------
# ç¿»è¯‘ï¼šArgosï¼ˆç¦»çº¿ï¼‰
# -------------------------
def _import_argos():
    try:
        import argostranslate.package  # noqa
        import argostranslate.translate  # noqa
        return True
    except Exception:
        return False


ARGOS_AVAILABLE = _import_argos()


def translate_argos(text: str, from_code: str, to_code: str) -> Optional[str]:
    """
    ä½¿ç”¨ Argos Translate ç¿»è¯‘ã€‚
    æ³¨æ„ï¼šArgos æ²¡æœ‰æ—¶ä¼šè¿”å› Noneï¼›æ¨¡å‹ç¼ºå¤±ä¹Ÿä¼šå¼‚å¸¸ -> None
    """
    if not text:
        return ""
    if not ARGOS_AVAILABLE:
        return None
    try:
        import argostranslate.translate as atranslate

        return normalize_ws(atranslate.translate(text, from_code, to_code))
    except Exception:
        return None


def translate_to_zh(text: str, src_lang: str) -> Optional[str]:
    """
    ç»Ÿä¸€ç¿»è¯‘åˆ°ä¸­æ–‡ï¼ˆzhï¼‰
    - en -> zhï¼šç›´æ¥
    - ja -> zhï¼šä¼˜å…ˆç›´æ¥ï¼›è‹¥å¤±è´¥åˆ™ ja->en å† en->zh
    """
    if not text:
        return ""
    text = normalize_ws(text)

    if src_lang == "en":
        return translate_argos(text, "en", "zh")

    if src_lang == "ja":
        direct = translate_argos(text, "ja", "zh")
        if direct:
            return direct
        # ä¸­è½¬ï¼šja -> en -> zh
        mid = translate_argos(text, "ja", "en")
        if not mid:
            return None
        return translate_argos(mid, "en", "zh")

    # å…¶ä»–è¯­è¨€ï¼šå…ˆä¸å¤„ç†
    return None


def install_argos_models() -> None:
    """
    Actions ä¸­å®‰è£…æ¨¡å‹ï¼š
    - en -> zh
    - ja -> en  ï¼ˆç”¨äº NHK ä¸­è½¬ï¼‰
    """
    if not ARGOS_AVAILABLE:
        log("âŒ æœªå®‰è£… argostranslateï¼Œæ— æ³•å®‰è£…æ¨¡å‹ã€‚è¯·å…ˆ pip install argostranslate")
        sys.exit(1)

    import argostranslate.package as ap

    log("ğŸŒ æ­£åœ¨æ›´æ–° Argos æ¨¡å‹ç´¢å¼•ï¼ˆéœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹ï¼‰...")
    ap.update_package_index()
    pkgs = ap.get_available_packages()

    wanted = {("en", "zh"), ("ja", "en")}
    installed = []

    for f, t in wanted:
        pkg = next((p for p in pkgs if p.from_code == f and p.to_code == t), None)
        if not pkg:
            log(f"âš ï¸ æœªåœ¨ç´¢å¼•ä¸­æ‰¾åˆ°ï¼š{f}->{t}")
            continue
        log(f"â¬‡ï¸ å‘ç°æ¨¡å‹ {f}->{t}ï¼Œå¼€å§‹ä¸‹è½½å¹¶å®‰è£…...")
        ap.install_from_path(pkg.download())
        installed.append(f"{f}->{t}")
        log(f"âœ… å·²å®‰è£…ï¼š{f}->{t}")

    if installed:
        log("âœ… æ¨¡å‹å®‰è£…å®Œæˆï¼š" + ", ".join(installed))
    else:
        log("âš ï¸ æœ¬æ¬¡æ²¡æœ‰å®‰è£…ä»»ä½•æ¨¡å‹ï¼ˆå¯èƒ½ç´¢å¼•ç¼ºå¤±æˆ–ç½‘ç»œé—®é¢˜ï¼‰")


# -------------------------
# æŠ“å–ç¬¬ä¸€æ®µæ‘˜è¦
# -------------------------
def http_get(url: str) -> Optional[str]:
    for i in range(RETRY):
        try:
            r = requests.get(url, headers=UA, timeout=DEFAULT_TIMEOUT)
            r.raise_for_status()
            r.encoding = r.apparent_encoding or r.encoding
            return r.text
        except Exception as e:
            if i < RETRY - 1:
                log(f"âš ï¸ æŠ“å–å¤±è´¥ï¼ˆç¬¬ {i+1}/{RETRY} æ¬¡ï¼‰ï¼š{e}")
                time.sleep(SLEEP_BETWEEN)
            else:
                log(f"âŒ æŠ“å–å¤±è´¥ï¼š{e}")
                return None
    return None


def extract_first_paragraph_bbc(html: str) -> str:
    soup = BeautifulSoup(html, "lxml")

    # BBC æ–°ç‰ˆå¸¸è§ç»“æ„ï¼šdata-component="text-block" é‡Œæœ‰ p
    candidates = []
    for p in soup.select('[data-component="text-block"] p'):
        t = normalize_ws(p.get_text(" ", strip=True))
        if len(t) >= 20:
            candidates.append(t)

    if not candidates:
        # fallbackï¼šå…¨ç«™ç¬¬ä¸€ä¸ªå¤Ÿé•¿çš„ p
        for p in soup.find_all("p"):
            t = normalize_ws(p.get_text(" ", strip=True))
            if len(t) >= 20:
                candidates.append(t)

    return candidates[0] if candidates else ""


def extract_first_paragraph_nhk(html: str) -> str:
    soup = BeautifulSoup(html, "lxml")

    # NHK å¸¸è§æ­£æ–‡å®¹å™¨ï¼šid/news_textbody æˆ– class åŒ…å« body
    candidates = []

    body = soup.find(id="news_textbody")
    if body:
        for p in body.find_all("p"):
            t = normalize_ws(p.get_text(" ", strip=True))
            if len(t) >= 15:
                candidates.append(t)

    if not candidates:
        # fallbackï¼šæ‰¾ main/article ä¸‹çš„ p
        for p in soup.select("article p, main p"):
            t = normalize_ws(p.get_text(" ", strip=True))
            if len(t) >= 15:
                candidates.append(t)

    return candidates[0] if candidates else ""


def fetch_first_paragraph(url: str, source_name: str) -> str:
    html = http_get(url)
    if not html:
        return ""
    if "bbc" in (url or "").lower() or source_name == "BBC News":
        return extract_first_paragraph_bbc(html)
    if "nhk" in (url or "").lower() or source_name == "NHKãƒ‹ãƒ¥ãƒ¼ã‚¹":
        return extract_first_paragraph_nhk(html)
    # fallback
    soup = BeautifulSoup(html, "lxml")
    for p in soup.find_all("p"):
        t = normalize_ws(p.get_text(" ", strip=True))
        if len(t) >= 20:
            return t
    return ""


# -------------------------
# æ•°æ®ç»“æ„
# -------------------------
@dataclass
class NewsItem:
    source: str
    source_lang: str
    title: str
    link: str
    published_at: str
    summary: str
    title_zh: str
    summary_zh: str


def item_to_dict(x: NewsItem) -> Dict[str, Any]:
    return {
        "source": x.source,
        "source_lang": x.source_lang,
        "title": x.title,
        "title_zh": x.title_zh,
        "link": x.link,
        "published_at": x.published_at,
        "summary": x.summary,
        "summary_zh": x.summary_zh,
    }


# -------------------------
# ä¸»æµç¨‹
# -------------------------
def fetch_all_entries() -> List[Tuple[Dict[str, Any], Dict[str, Any]]]:
    """
    è¿”å› [(source_config, entry_dict), ...]
    """
    all_entries = []
    for src in SOURCES:
        log(f"ğŸ“° æ­£åœ¨æŠ“å– {src['name']}ï¼š{src['rss']}")
        feed = feedparser.parse(src["rss"])
        if feed.bozo:
            log(f"âš ï¸ RSS è§£æè­¦å‘Šï¼š{getattr(feed, 'bozo_exception', '')}")
        entries = feed.entries or []
        log(f"âœ… {src['name']} æŠ“å–æˆåŠŸï¼Œè§£æåˆ° {len(entries)} æ¡æ¡ç›®")
        for e in entries:
            all_entries.append((src, e))
    return all_entries


def dedup_entries(entries: List[Tuple[Dict[str, Any], Any]]) -> List[Tuple[Dict[str, Any], Any]]:
    seen = set()
    out = []
    for src, e in entries:
        link = (getattr(e, "link", None) or e.get("link") or "").strip()
        if not link:
            continue
        if link in seen:
            continue
        seen.add(link)
        out.append((src, e))
    return out


def sort_entries(entries: List[Tuple[Dict[str, Any], Any]]) -> List[Tuple[Dict[str, Any], Any]]:
    def key_fn(pair):
        src, e = pair
        dt = parse_dt(e)
        return dt.timestamp() if dt else 0.0

    return sorted(entries, key=key_fn, reverse=True)


def build_items(entries: List[Tuple[Dict[str, Any], Any]], limit: int) -> List[NewsItem]:
    entries = sort_entries(entries)[:limit]

    log(f"ğŸ§¾ æ­£åœ¨ä¸ºæœ¬æ¬¡è¾“å‡ºçš„ {len(entries)} æ¡æ–°é—»æŠ“å–â€œç¬¬ä¸€æ®µæ‘˜è¦â€...")
    items: List[NewsItem] = []
    for i, (src, e) in enumerate(entries, 1):
        title = normalize_ws((getattr(e, "title", None) or e.get("title") or "").strip())
        link = (getattr(e, "link", None) or e.get("link") or "").strip()
        dt = parse_dt(e)
        published_at = fmt_dt(dt)

        log(f"   [{i}/{len(entries)}] æŠ“æ‘˜è¦ï¼š{link}")
        first_para = fetch_first_paragraph(link, src["name"])
        first_para = normalize_ws(first_para)

        # ç¿»è¯‘ï¼ˆæ ‡é¢˜ + æ‘˜è¦ï¼‰
        title_zh = translate_to_zh(title, src["lang"]) or "ï¼ˆæœªç¿»è¯‘/ç¿»è¯‘å¤±è´¥ï¼‰"
        summary_zh = translate_to_zh(first_para, src["lang"]) or "ï¼ˆæœªç¿»è¯‘/ç¿»è¯‘å¤±è´¥ï¼‰"

        items.append(
            NewsItem(
                source=src["name"],
                source_lang=src["lang"],
                title=title,
                link=link,
                published_at=published_at,
                summary=first_para,
                title_zh=title_zh,
                summary_zh=summary_zh,
            )
        )
    return items


def render_terminal(items: List[NewsItem], n: int) -> None:
    show = items[:n]
    log("")
    log(f"ğŸ“Œ ç»ˆç«¯å±•ç¤ºæœ€æ–° {len(show)} æ¡ï¼š")
    log("-" * 60)
    for idx, it in enumerate(show, 1):
        log(f"{idx}. [{it.published_at}] ({it.source})")
        log(f"   æ ‡é¢˜ï¼š{it.title}")
        log(f"   æ ‡é¢˜ï¼ˆä¸­æ–‡ï¼‰ï¼š{it.title_zh}")
        log(f"   é“¾æ¥ï¼š{it.link}")
        log(f"   æ‘˜è¦ï¼ˆç¬¬ä¸€æ®µï¼‰ï¼š{it.summary}")
        log(f"   æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰ï¼š{it.summary_zh}")
        log("")
    log("-" * 60)


def write_site_data(items: List[NewsItem]) -> None:
    safe_mkdir("docs")

    now = datetime.now(timezone.utc).astimezone()
    payload = {
        "site_title": "Michael News",
        "generated_at": now.isoformat(timespec="seconds"),
        "count": len(items),
        "items": [item_to_dict(x) for x in items],
    }
    with open(DATA_OUT_PATH, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    log(f"ğŸ’¾ å·²ç”Ÿæˆç«™ç‚¹æ•°æ®ï¼š{DATA_OUT_PATH}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--install-models", action="store_true", help="å®‰è£… Argos ç¿»è¯‘æ¨¡å‹ï¼ˆActions ç”¨ï¼‰")
    ap.add_argument("--all", action="store_true", help="è¾“å‡ºå…¨éƒ¨ï¼ˆä¸åšå¢é‡ï¼‰")
    ap.add_argument("--new", action="store_true", help="ï¼ˆä¿ç•™å‚æ•°ï¼Œä½†æ­¤ç²¾ç®€ç‰ˆæœ¬ä¸åšå¢é‡ï¼‰")
    ap.add_argument("--limit", type=int, default=50, help="æœ€å¤šå¤„ç†å¤šå°‘æ¡ï¼ˆé»˜è®¤ 50ï¼‰")
    ap.add_argument("--build-site", action="store_true", help="ç”Ÿæˆ docs/data.jsonï¼ˆç”¨äº GitHub Pagesï¼‰")
    ap.add_argument("--print", action="store_true", help="ç»ˆç«¯æ‰“å°æœ€æ–° 3 æ¡ï¼ˆé»˜è®¤ä¸å¼€ï¼‰")
    args = ap.parse_args()

    if args.install_models:
        install_argos_models()
        return

    entries = fetch_all_entries()
    entries = dedup_entries(entries)

    # è¿™ä¸ªç²¾ç®€ç‰ˆé»˜è®¤ä¸åšå¢é‡ï¼Œ--new åªæ˜¯å…¼å®¹ä½ åŸæ¥çš„å‘½ä»¤
    entries = sort_entries(entries)
    items = build_items(entries, limit=args.limit)

    if args.build_site:
        write_site_data(items)

    if args.print:
        render_terminal(items, n=min(3, len(items)))


if __name__ == "__main__":
    main()
