# -*- coding: utf-8 -*-
"""
fetch_news.py
========================================
RSS æ–°é—»æŠ“å– + ç¬¬ä¸€æ®µæ‘˜è¦ + ä¸­æ–‡ç¿»è¯‘ï¼ˆç¦»çº¿ Argosï¼‰

- RSSï¼š
  - BBC World: http://newsrss.bbc.co.uk/rss/newsonline_uk_edition/world/rss.xml
  - NHK News(cat0): https://www3.nhk.or.jp/rss/news/cat0.xml

è¾“å‡ºï¼š
- ç»ˆç«¯æ‰“å°ï¼ˆå¯ --limit æ§åˆ¶ï¼‰
- å†™å…¥ output/news_YYYYMMDD.json
- å¯é€‰å†™å…¥ç«™ç‚¹æ•°æ® docs/news.jsonï¼ˆç”¨äº GitHub Pagesï¼‰

ç¿»è¯‘ï¼š
- è‹±æ–‡ï¼šen->zh
- æ—¥æ–‡ï¼šja->en->zhï¼ˆé“¾å¼ç¿»è¯‘ï¼Œè§£å†³ NHK æ­£æ–‡ä¸ç¿»è¯‘çš„é—®é¢˜ï¼‰

å‘½ä»¤ï¼š
- å®‰è£…ç¦»çº¿ç¿»è¯‘æ¨¡å‹ï¼ˆéœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹åŒ…ï¼‰ï¼š
    python fetch_news.py --install-models
- åªè¾“å‡ºæ–°å¢ï¼ˆé»˜è®¤ï¼‰ï¼š
    python fetch_news.py --new --limit 3
- è¾“å‡ºå…¨éƒ¨ï¼š
    python fetch_news.py --all --limit 3
- ç”Ÿæˆç½‘ç«™ç”¨æ•°æ®ï¼š
    python fetch_news.py --all --site
"""

import argparse
import json
import os
import re
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import feedparser
import requests
from bs4 import BeautifulSoup
from dateutil import parser as date_parser


# =========================
# 1) å¯é…ç½®é¡¹ï¼ˆæ–°æ‰‹å»ºè®®åªæ”¹è¿™é‡Œï¼‰
# =========================

RSS_FEEDS = [
    {
        "name": "BBC World",
        "url": "http://newsrss.bbc.co.uk/rss/newsonline_uk_edition/world/rss.xml"
    },
    {
        "name": "NHK News (cat0)",
        "url": "https://www3.nhk.or.jp/rss/news/cat0.xml"
    },
]

OUTPUT_DIR = "output"
DEFAULT_PRINT_LIMIT = 20

REQUEST_TIMEOUT_SECONDS = 12
REQUEST_RETRY_TIMES = 2
REQUEST_RETRY_SLEEP_SECONDS = 1

# æŠ“æ–‡ç« ç¬¬ä¸€æ®µçš„â€œèŠ‚æµâ€ï¼Œé¿å…è¢«å°ï¼ˆå»ºè®® 0.2~1.0ï¼‰
ARTICLE_FETCH_SLEEP_SECONDS = 0.35

# å¢é‡è®°å½•
SEEN_FILE = "seen.json"

# ç¿»è¯‘ç¼“å­˜ï¼ˆé¿å…é‡å¤ç¿»ï¼‰
TRANSLATE_CACHE_FILE = "translation_cache.json"


# =========================
# 2) é€šç”¨å·¥å…·å‡½æ•°
# =========================

def print_cn(msg: str) -> None:
    print(msg)


def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)


def safe_get_str(value, default: str = "") -> str:
    if value is None:
        return default
    try:
        s = str(value).strip()
        return s if s else default
    except Exception:
        return default


def load_json(path: str, default):
    if not os.path.exists(path):
        return default
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default


def save_json(path: str, data) -> None:
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print_cn(f"âš ï¸ å†™å…¥å¤±è´¥ï¼š{path}ï¼ˆåŸå› ï¼š{e}ï¼‰")


def load_seen(file_path: str) -> Set[str]:
    data = load_json(file_path, {"seen": []})
    seen_list = data.get("seen", [])
    if not isinstance(seen_list, list):
        return set()
    return set(str(x) for x in seen_list)


def save_seen(file_path: str, seen_set: Set[str]) -> None:
    save_json(file_path, {"seen": sorted(seen_set)})


def parse_datetime_from_entry(entry: dict) -> Optional[datetime]:
    # feedparser çš„ parsed å­—æ®µ
    for key in ("published_parsed", "updated_parsed"):
        parsed = entry.get(key)
        if parsed:
            try:
                ts = time.mktime(parsed)
                return datetime.fromtimestamp(ts).astimezone()
            except Exception:
                pass

    # æ–‡æœ¬å­—æ®µ
    for key in ("published", "updated"):
        text = entry.get(key)
        if text:
            try:
                dt = date_parser.parse(str(text))
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=datetime.now().astimezone().tzinfo)
                return dt.astimezone()
            except Exception:
                pass

    return None


def requests_get_with_retry(url: str, timeout: int, retry_times: int, retry_sleep: int) -> Optional[requests.Response]:
    headers = {
        "User-Agent": "michael-news-bot/1.0 (+rss fetcher)"
    }

    attempt_total = retry_times + 1
    for attempt in range(1, attempt_total + 1):
        try:
            resp = requests.get(url, headers=headers, timeout=timeout)
            resp.raise_for_status()
            return resp
        except requests.exceptions.RequestException as e:
            if attempt < attempt_total:
                print_cn(f"âš ï¸ æŠ“å–å¤±è´¥ï¼ˆç¬¬ {attempt}/{attempt_total} æ¬¡ï¼‰ï¼š{e}")
                print_cn(f"   {retry_sleep} ç§’åé‡è¯•...")
                time.sleep(retry_sleep)
            else:
                print_cn(f"âŒ æŠ“å–å¤±è´¥ï¼ˆå·²é‡è¯• {retry_times} æ¬¡ä»å¤±è´¥ï¼‰ï¼š{e}")
                return None
    return None


def build_item_key(title: str, link: str) -> str:
    return link if link else title


def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\s+", " ", s)
    return s.strip()


def make_output_filename(output_dir: str, fmt: str = "json") -> str:
    ensure_dir(output_dir)
    date_str = datetime.now().strftime("%Y%m%d")
    base = f"news_{date_str}.{fmt}"
    path = os.path.join(output_dir, base)
    if not os.path.exists(path):
        return path
    time_str = datetime.now().strftime("%H%M%S")
    base2 = f"news_{date_str}_{time_str}.{fmt}"
    return os.path.join(output_dir, base2)


# =========================
# 3) ç½‘é¡µæ­£æ–‡ç¬¬ä¸€æ®µæå–
# =========================

def extract_first_paragraph(url: str, html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")

    # å»æ‰è„šæœ¬/æ ·å¼
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    host = urlparse(url).netloc.lower()

    def pick_first_p(container) -> str:
        if not container:
            return ""
        ps = container.find_all("p")
        for p in ps:
            t = normalize_text(p.get_text(" ", strip=True))
            # è¿‡æ»¤å¤ªçŸ­/åƒå¯¼èˆªçš„
            if len(t) >= 25 and "cookie" not in t.lower():
                return t
        return ""

    # --- NHK ---
    if "nhk.or.jp" in host:
        candidates = [
            soup.select_one("#js-article-body"),
            soup.select_one(".content--detail-body"),
            soup.select_one("article"),
            soup.select_one("main"),
        ]
        for c in candidates:
            t = pick_first_p(c)
            if t:
                return t

    # --- BBC ---
    if "bbc." in host:
        c = soup.select_one("article") or soup.select_one("main")
        t = pick_first_p(c)
        if t:
            return t

    # é€šç”¨å…œåº•
    c = soup.select_one("article") or soup.select_one("main") or soup
    t = pick_first_p(c)
    if t:
        return t

    # å†å…œåº•ï¼šå…¨ç«™ç¬¬ä¸€æ®µ
    for p in soup.find_all("p"):
        t = normalize_text(p.get_text(" ", strip=True))
        if len(t) >= 25:
            return t

    return ""


def fetch_first_paragraph(url: str) -> str:
    if not url:
        return ""
    resp = requests_get_with_retry(
        url=url,
        timeout=REQUEST_TIMEOUT_SECONDS,
        retry_times=REQUEST_RETRY_TIMES,
        retry_sleep=REQUEST_RETRY_SLEEP_SECONDS
    )
    if resp is None:
        return ""
    html = resp.text
    return extract_first_paragraph(url, html)


# =========================
# 4) ç¦»çº¿ç¿»è¯‘ï¼ˆArgosï¼‰
# =========================

ARGOS_AVAILABLE = False
try:
    import argostranslate.package
    import argostranslate.translate
    ARGOS_AVAILABLE = True
except Exception:
    ARGOS_AVAILABLE = False


def load_translate_cache() -> Dict[str, str]:
    data = load_json(TRANSLATE_CACHE_FILE, {})
    if not isinstance(data, dict):
        return {}
    # é™åˆ¶ä¸€ä¸‹å¤§å°ï¼Œé¿å…æ— é™å¢é•¿ï¼ˆå¯è‡ªè¡Œè°ƒï¼‰
    if len(data) > 30000:
        # ç®€å•è£å‰ªï¼šåªä¿ç•™æœ€åä¸€éƒ¨åˆ†ï¼ˆéä¸¥æ ¼ LRUï¼Œä½†å¤Ÿç”¨ï¼‰
        items = list(data.items())[-20000:]
        return dict(items)
    return data


def save_translate_cache(cache: Dict[str, str]) -> None:
    save_json(TRANSLATE_CACHE_FILE, cache)


def detect_lang_simple(text: str) -> str:
    """å¤Ÿç”¨çš„ç²—æ£€æµ‹ï¼šæœ‰å‡å => jaï¼›è‹±æ–‡å æ¯”é«˜ => enï¼›å¦åˆ™é»˜è®¤ jaï¼ˆNHK å¤šä¸ºæ±‰å­—+å‡åï¼‰"""
    if not text:
        return "unknown"
    for ch in text:
        code = ord(ch)
        if (0x3040 <= code <= 0x309F) or (0x30A0 <= code <= 0x30FF):
            return "ja"
    ascii_letters = sum(('a' <= c.lower() <= 'z') for c in text)
    if ascii_letters >= max(10, int(len(text) * 0.2)):
        return "en"
    return "ja"


def argos_has_pair(from_code: str, to_code: str) -> bool:
    if not ARGOS_AVAILABLE:
        return False
    langs = argostranslate.translate.get_installed_languages()
    src = next((l for l in langs if l.code == from_code), None)
    if not src:
        return False
    return any(t.code == to_code for t in src.translations)


def argos_translate(text: str, from_code: str, to_code: str) -> str:
    if not ARGOS_AVAILABLE or not text.strip():
        return ""
    try:
        langs = argostranslate.translate.get_installed_languages()
        src = next((l for l in langs if l.code == from_code), None)
        if not src:
            return ""
        tr = next((t for t in src.translations if t.code == to_code), None)
        if not tr:
            return ""
        return tr.translate(text)
    except Exception:
        return ""


def translate_to_zh(text: str, cache: Dict[str, str]) -> str:
    """
    å…è´¹ç¦»çº¿ç¿»è¯‘ï¼š
    - en -> zh
    - ja -> zhï¼šä¼˜å…ˆ directï¼ˆå¦‚æœæœªæ¥æœ‰ï¼‰ï¼Œå¦åˆ™ ja->en->zh
    """
    if not text or not text.strip():
        return ""

    lang = detect_lang_simple(text)
    key = f"{lang}::zh::{text}"
    if key in cache:
        return cache[key]

    # è‹±æ–‡ç›´ç¿»
    if lang == "en":
        out = argos_translate(text, "en", "zh")
        out = (out or "").strip()
        cache[key] = out
        return out

    # æ—¥æ–‡ï¼šdirect æˆ–é“¾å¼
    if lang == "ja":
        if argos_has_pair("ja", "zh"):
            out = argos_translate(text, "ja", "zh")
            out = (out or "").strip()
            cache[key] = out
            return out

        # é“¾å¼ï¼šja -> en -> zh
        mid_key = f"ja::en::{text}"
        if mid_key in cache:
            mid = cache[mid_key]
        else:
            mid = argos_translate(text, "ja", "en")
            mid = (mid or "").strip()
            cache[mid_key] = mid

        if not mid:
            cache[key] = ""
            return ""

        out_key2 = f"en::zh::{mid}"
        if out_key2 in cache:
            out = cache[out_key2]
        else:
            out = argos_translate(mid, "en", "zh")
            out = (out or "").strip()
            cache[out_key2] = out

        cache[key] = out
        return out

    # å…œåº•
    out = argos_translate(text, "en", "zh")
    out = (out or "").strip()
    cache[key] = out
    return out


def install_argos_models() -> int:
    """
    å®‰è£… Argos ç¦»çº¿æ¨¡å‹ï¼ˆéœ€è¦è”ç½‘ä¸‹è½½ï¼‰
    æˆ‘ä»¬éœ€è¦ï¼š
      - en -> zhï¼ˆBBCï¼‰
      - ja -> enï¼ˆNHK é“¾å¼ç¿»è¯‘ç¬¬ä¸€æ®µ/æ ‡é¢˜ï¼‰
    """
    if not ARGOS_AVAILABLE:
        print_cn("âŒ ä½ è¿˜æ²¡å®‰è£… argostranslate æˆ–å¯¼å…¥å¤±è´¥ã€‚")
        print_cn("   è§£å†³ï¼špython -m pip install argostranslate")
        return 1

    print_cn("ğŸŒ æ­£åœ¨æ›´æ–° Argos æ¨¡å‹ç´¢å¼•ï¼ˆéœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹ï¼‰...")
    try:
        argostranslate.package.update_package_index()
        available = argostranslate.package.get_available_packages()
    except Exception as e:
        print_cn(f"âŒ æ›´æ–°/è¯»å–æ¨¡å‹ç´¢å¼•å¤±è´¥ï¼š{e}")
        return 1

    need_pairs = [("en", "zh"), ("ja", "en")]

    installed_any = False
    for src, dst in need_pairs:
        if argos_has_pair(src, dst):
            print_cn(f"âœ… å·²å­˜åœ¨æ¨¡å‹ï¼š{src}->{dst}")
            continue

        pkg = next((p for p in available if p.from_code == src and p.to_code == dst), None)
        if not pkg:
            print_cn(f"âš ï¸ æœªåœ¨ç´¢å¼•ä¸­æ‰¾åˆ°ï¼š{src}->{dst}")
            continue

        try:
            print_cn(f"â¬‡ï¸ å‘ç°æ¨¡å‹ {src}->{dst}ï¼Œå¼€å§‹ä¸‹è½½å¹¶å®‰è£…...")
            download_path = pkg.download()
            argostranslate.package.install_from_path(download_path)
            print_cn(f"âœ… å·²å®‰è£…ï¼š{src}->{dst}")
            installed_any = True
        except Exception as e:
            print_cn(f"âŒ å®‰è£…å¤±è´¥ï¼š{src}->{dst}ï¼ŒåŸå› ï¼š{e}")

    if not installed_any:
        print_cn("âœ… æ¨¡å‹æ£€æŸ¥å®Œæˆï¼ˆæ²¡æœ‰æ–°å¢å®‰è£…ä¹Ÿæ²¡å…³ç³»ï¼‰ã€‚")
    else:
        print_cn("âœ… æ¨¡å‹å®‰è£…å®Œæˆã€‚")

    return 0


# =========================
# 5) RSS æŠ“å–ã€åˆå¹¶ã€å»é‡ã€å¢é‡
# =========================

def fetch_and_parse_one_feed(feed_name: str, feed_url: str) -> List[Dict]:
    print_cn(f"ğŸ“° æ­£åœ¨æŠ“å– {feed_name}ï¼š{feed_url}")

    resp = requests_get_with_retry(
        url=feed_url,
        timeout=REQUEST_TIMEOUT_SECONDS,
        retry_times=REQUEST_RETRY_TIMES,
        retry_sleep=REQUEST_RETRY_SLEEP_SECONDS
    )
    if resp is None:
        print_cn(f"âš ï¸ è·³è¿‡ {feed_name}ï¼ˆæŠ“å–å¤±è´¥ï¼‰")
        return []

    parsed = feedparser.parse(resp.content)

    feed_title = safe_get_str(parsed.get("feed", {}).get("title"), default=feed_name)
    source_name = feed_title if feed_title else feed_name

    entries = parsed.get("entries", [])
    print_cn(f"âœ… {feed_name} æŠ“å–æˆåŠŸï¼Œè§£æåˆ° {len(entries)} æ¡æ¡ç›®")

    now_dt = datetime.now().astimezone()
    items: List[Dict] = []

    for entry in entries:
        title = safe_get_str(entry.get("title"), default="(æ— æ ‡é¢˜)")
        link = safe_get_str(entry.get("link"), default="")
        dt = parse_datetime_from_entry(entry) or now_dt

        published_str = dt.strftime("%Y-%m-%d %H:%M:%S%z")
        if len(published_str) >= 5:
            published_str = published_str[:-5] + published_str[-5:-2] + ":" + published_str[-2:]

        item_key = build_item_key(title=title, link=link)

        items.append({
            "title": title,
            "link": link,
            "published": published_str,
            "source": source_name,
            "_published_ts": dt.timestamp(),
            "_key": item_key,
        })

    return items


def merge_sort_dedupe(items: List[Dict]) -> List[Dict]:
    items_sorted = sorted(items, key=lambda x: x.get("_published_ts", 0), reverse=True)

    seen_in_run: Set[str] = set()
    unique_items: List[Dict] = []

    for it in items_sorted:
        key = safe_get_str(it.get("_key"), default="")
        if not key:
            key = f"__empty__{it.get('_published_ts', 0)}"

        if key in seen_in_run:
            continue

        seen_in_run.add(key)
        unique_items.append(it)

    return unique_items


def filter_new_items(items: List[Dict], seen_before: Set[str], mode_new: bool) -> Tuple[List[Dict], Set[str]]:
    updated_seen = set(seen_before)

    if not mode_new:
        for it in items:
            k = safe_get_str(it.get("_key"), default="")
            if k:
                updated_seen.add(k)
        return items, updated_seen

    new_items: List[Dict] = []
    for it in items:
        k = safe_get_str(it.get("_key"), default="")
        if not k:
            continue

        if k not in seen_before:
            new_items.append(it)
        updated_seen.add(k)

    return new_items, updated_seen


def cleanup_internal_fields(items: List[Dict]) -> List[Dict]:
    cleaned: List[Dict] = []
    for it in items:
        cleaned.append({
            "title": it.get("title", ""),
            "title_zh": it.get("title_zh", ""),
            "snippet": it.get("snippet", ""),
            "snippet_zh": it.get("snippet_zh", ""),
            "link": it.get("link", ""),
            "published": it.get("published", ""),
            "source": it.get("source", ""),
        })
    return cleaned


def write_json(file_path: str, items: List[Dict]) -> None:
    ensure_dir(os.path.dirname(file_path) or ".")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)


def print_items(items: List[Dict], limit: int) -> None:
    if not items:
        print_cn("ï¼ˆæœ¬æ¬¡æ²¡æœ‰éœ€è¦è¾“å‡ºçš„æ–°é—»ï¼‰")
        return

    print_cn("")
    print_cn(f"ğŸ“Œ ç»ˆç«¯å±•ç¤ºæœ€æ–° {min(limit, len(items))} æ¡ï¼š")
    print_cn("------------------------------------------------------------")

    for idx, it in enumerate(items[:limit], start=1):
        print_cn(f"{idx}. [{it.get('published', '')}] ({it.get('source', '')})")
        print_cn(f"   æ ‡é¢˜ï¼š{it.get('title', '')}")
        tz = safe_get_str(it.get("title_zh"), "")
        print_cn(f"   æ ‡é¢˜ï¼ˆä¸­æ–‡ï¼‰ï¼š{tz if tz else 'ï¼ˆæœªç¿»è¯‘/ç¿»è¯‘å¤±è´¥ï¼‰'}")
        print_cn(f"   é“¾æ¥ï¼š{it.get('link', '')}")

        sn = safe_get_str(it.get("snippet"), "")
        snz = safe_get_str(it.get("snippet_zh"), "")

        if sn:
            print_cn(f"   æ‘˜è¦ï¼ˆç¬¬ä¸€æ®µï¼‰ï¼š{sn}")
            print_cn(f"   æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰ï¼š{snz if snz else 'ï¼ˆæœªç¿»è¯‘/ç¿»è¯‘å¤±è´¥ï¼‰'}")
        else:
            print_cn("   æ‘˜è¦ï¼ˆç¬¬ä¸€æ®µï¼‰ï¼šï¼ˆæœªæå–åˆ°ï¼Œå¯èƒ½æ˜¯ç½‘ç«™ç»“æ„å˜åŒ–/åçˆ¬/ç½‘ç»œé—®é¢˜ï¼‰")
            print_cn("   æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰ï¼šï¼ˆæœªç¿»è¯‘/ç¿»è¯‘å¤±è´¥ï¼‰")

        print_cn("")

    print_cn("------------------------------------------------------------")


# =========================
# 6) å‘½ä»¤è¡Œå…¥å£
# =========================

def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="æŠ“å– RSS æ–°é—»ï¼ˆç¬¬ä¸€æ®µæ‘˜è¦ + ä¸­æ–‡ç¿»è¯‘ + ç«™ç‚¹è¾“å‡ºï¼‰")

    group = parser.add_mutually_exclusive_group()
    group.add_argument("--new", action="store_true", help="åªè¾“å‡ºæ–°å¢ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰")
    group.add_argument("--all", action="store_true", help="è¾“å‡ºå…¨éƒ¨ï¼ˆä¸åšå¢é‡è¿‡æ»¤ï¼‰")

    parser.add_argument("--limit", type=int, default=DEFAULT_PRINT_LIMIT, help="ç»ˆç«¯æ‰“å°æ¡æ•°ï¼ˆé»˜è®¤ 20ï¼‰")
    parser.add_argument("--site", action="store_true", help="ç”Ÿæˆç«™ç‚¹ç”¨ docs/news.jsonï¼ˆå»ºè®®é…åˆ --allï¼‰")
    parser.add_argument("--install-models", action="store_true", help="å®‰è£… Argos ç¦»çº¿ç¿»è¯‘æ¨¡å‹ï¼ˆéœ€è”ç½‘ä¸‹è½½ï¼‰")

    return parser.parse_args()


def main() -> None:
    args = parse_args()

    if args.install_models:
        sys.exit(install_argos_models())

    mode_new = True
    if args.all:
        mode_new = False

    seen_before = load_seen(SEEN_FILE)

    all_items: List[Dict] = []
    for feed in RSS_FEEDS:
        name = safe_get_str(feed.get("name"), default="(æœªå‘½åRSS)")
        url = safe_get_str(feed.get("url"), default="")
        items = fetch_and_parse_one_feed(feed_name=name, feed_url=url)
        all_items.extend(items)

    if not all_items:
        print_cn("âŒ æ²¡æœ‰æŠ“åˆ°ä»»ä½•æ¡ç›®ã€‚è¯·æ£€æŸ¥ç½‘ç»œ/RSS é“¾æ¥æ˜¯å¦å¯è®¿é—®ã€‚")
        return

    merged_unique = merge_sort_dedupe(all_items)
    print_cn(f"ğŸ” åˆå¹¶åå»é‡ï¼š{len(merged_unique)} æ¡ï¼ˆæ¥è‡ª {len(RSS_FEEDS)} ä¸ªæºï¼‰")

    selected_items, updated_seen = filter_new_items(
        items=merged_unique,
        seen_before=seen_before,
        mode_new=mode_new
    )

    if mode_new:
        print_cn(f"ğŸ†• æ–°å¢æ–°é—»ï¼š{len(selected_items)} æ¡ï¼ˆé»˜è®¤åªè¾“å‡ºæ–°å¢ï¼‰")
    else:
        print_cn(f"ğŸ“¦ è¾“å‡ºå…¨éƒ¨ï¼š{len(selected_items)} æ¡ï¼ˆä¸åšå¢é‡ï¼‰")

    # æŠ“ç¬¬ä¸€æ®µæ‘˜è¦
    if selected_items:
        print_cn(f"ğŸ§¾ æ­£åœ¨ä¸ºæœ¬æ¬¡è¾“å‡ºçš„ {len(selected_items)} æ¡æ–°é—»æŠ“å–â€œç¬¬ä¸€æ®µæ‘˜è¦â€...")
        for i, it in enumerate(selected_items, start=1):
            link = safe_get_str(it.get("link"), "")
            if not link:
                it["snippet"] = ""
                continue
            print_cn(f"   [{i}/{len(selected_items)}] æŠ“æ‘˜è¦ï¼š{link}")
            it["snippet"] = fetch_first_paragraph(link)
            time.sleep(ARTICLE_FETCH_SLEEP_SECONDS)

    # ç¿»è¯‘
    if ARGOS_AVAILABLE:
        cache = load_translate_cache()
        # åªæœ‰åœ¨ç¡®å®å®‰è£…äº† en->zh æ—¶æ‰ç¿»è¯‘ï¼ˆå¦åˆ™å…¨æ˜¯ç©ºï¼‰
        if argos_has_pair("en", "zh"):
            print_cn("ğŸŒ æ­£åœ¨æŠŠæ ‡é¢˜ä¸æ‘˜è¦ç¿»è¯‘æˆä¸­æ–‡ï¼ˆç¦»çº¿ Argosï¼‰...")
            for i, it in enumerate(selected_items, start=1):
                title = safe_get_str(it.get("title"), "")
                snip = safe_get_str(it.get("snippet"), "")
                if title:
                    print_cn(f"   [{i}/{len(selected_items)}] ç¿»è¯‘æ ‡é¢˜ï¼š{title[:40]}...")
                    it["title_zh"] = translate_to_zh(title, cache)
                else:
                    it["title_zh"] = ""
                if snip:
                    it["snippet_zh"] = translate_to_zh(snip, cache)
                else:
                    it["snippet_zh"] = ""
            save_translate_cache(cache)
        else:
            print_cn("âš ï¸ ä½ è¿˜æ²¡å®‰è£… Argos çš„ en->zh æ¨¡å‹ï¼Œå°†è·³è¿‡ç¿»è¯‘ã€‚")
            print_cn("   è¿è¡Œï¼špython fetch_news.py --install-models")
            for it in selected_items:
                it["title_zh"] = ""
                it["snippet_zh"] = ""
    else:
        print_cn("âš ï¸ æœªæ£€æµ‹åˆ° argostranslateï¼Œå°†è·³è¿‡ç¿»è¯‘ã€‚")
        print_cn("   è§£å†³ï¼špython -m pip install argostranslate")
        for it in selected_items:
            it["title_zh"] = ""
            it["snippet_zh"] = ""

    output_items = cleanup_internal_fields(selected_items)

    # ä¿å­˜ output æ–‡ä»¶
    ensure_dir(OUTPUT_DIR)
    out_path = make_output_filename(OUTPUT_DIR, "json")
    write_json(out_path, output_items)
    print_cn(f"ğŸ’¾ å·²ä¿å­˜åˆ°ï¼š{out_path}")

    # ä¿å­˜ç«™ç‚¹æ•°æ® docs/news.jsonï¼ˆå»ºè®®ç”¨äº GitHub Pagesï¼‰
    if args.site:
        ensure_dir("docs")
        site_path = os.path.join("docs", "news.json")
        write_json(site_path, output_items)
        print_cn(f"ğŸŒ å·²ç”Ÿæˆç«™ç‚¹æ•°æ®ï¼š{site_path}")

    save_seen(SEEN_FILE, updated_seen)

    limit = max(1, int(args.limit))
    print_items(output_items, limit=limit)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print_cn("\nğŸ›‘ ä½ æ‰‹åŠ¨ä¸­æ–­äº†ç¨‹åºï¼ˆCtrl+Cï¼‰")
        sys.exit(0)
    except Exception as e:
        print_cn(f"\nâŒ ç¨‹åºå‘ç”Ÿæœªæ•è·å¼‚å¸¸ï¼š{e}")
        print_cn("æŠŠä¸Šé¢çš„æŠ¥é”™å¤åˆ¶ç»™æˆ‘ï¼Œæˆ‘èƒ½ç»§ç»­å¸®ä½ ä¿®ã€‚")
        sys.exit(1)



