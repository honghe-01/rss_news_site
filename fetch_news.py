# -*- coding: utf-8 -*-
"""
Michael News - RSS æ–°é—»ç«™ç‚¹ç”Ÿæˆå™¨ï¼ˆæç®€ç‰ˆï¼‰
========================================
åŠŸèƒ½ï¼š
1) æŠ“å– BBC World + NHK(cat0) RSS
2) è®¿é—®æ¯æ¡æ–°é—» linkï¼Œæå–â€œç¬¬ä¸€æ®µâ€
3) ç¦»çº¿ç¿»è¯‘æˆä¸­æ–‡ï¼ˆæ— éœ€ API Keyï¼‰ï¼š
   - è‹±æ–‡ï¼šen -> zh
   - æ—¥æ–‡ï¼šja -> en -> zhï¼ˆç”¨ä¸­è½¬ï¼Œé¿å…æ‰¾ä¸åˆ° ja->zh æ¨¡å‹ï¼‰
4) ç”Ÿæˆé™æ€ç½‘é¡µ docs/index.htmlï¼ˆGitHub Pages å¯ç›´æ¥å±•ç¤ºï¼‰
5) æ¯æ¡æ–°é—»ä»…å±•ç¤ºï¼š
   æ ‡é¢˜åŸæ–‡ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰
   ç¬¬ä¸€æ®µåŸæ–‡ï¼ˆä¸­æ–‡ç¿»è¯‘ï¼‰

ç”¨æ³•ï¼š
- æœ¬åœ°è·‘ï¼š
    python fetch_news.py --all
    python fetch_news.py --new
- å®‰è£…ç¦»çº¿æ¨¡å‹ï¼ˆåœ¨ GitHub Actions é‡Œä¹Ÿä¼šè·‘ï¼‰ï¼š
    python fetch_news.py --install-models
"""

import argparse
import json
import os
import re
import sys
import time
import html
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Set, Tuple
from urllib.parse import urlparse

import feedparser
import requests
from dateutil import parser as date_parser
from bs4 import BeautifulSoup

# -------------------------
# é…ç½®ï¼ˆä½ åªéœ€è¦æ”¹è¿™é‡Œï¼‰
# -------------------------
RSS_FEEDS = [
    {"name": "BBC News", "url": "http://newsrss.bbc.co.uk/rss/newsonline_uk_edition/world/rss.xml"},
    {"name": "NHKãƒ‹ãƒ¥ãƒ¼ã‚¹", "url": "https://www3.nhk.or.jp/rss/news/cat0.xml"},
]

OUTPUT_DIR = "output"
SITE_DIR = "docs"  # GitHub Pages ä½¿ç”¨ /docs
SEEN_FILE = "seen.json"
TRANSLATION_CACHE_FILE = "translation_cache.json"

REQUEST_TIMEOUT_SECONDS = 12
REQUEST_RETRY_TIMES = 2
REQUEST_RETRY_SLEEP_SECONDS = 1
ARTICLE_FETCH_SLEEP_SECONDS = 0.25

# ç«™ç‚¹æ˜¾ç¤ºæ¡æ•°ï¼ˆç½‘é¡µä¼šæ˜¾ç¤ºå…¨éƒ¨ï¼›ç»ˆç«¯å¯ç”¨ --limit æ§åˆ¶ï¼‰
DEFAULT_PRINT_LIMIT = 10

# Tokyo æ—¶åŒºæ˜¾ç¤ºï¼ˆGitHub Actions é»˜è®¤ UTCï¼Œè¿™é‡Œå¼ºåˆ¶è½¬ JSTï¼‰
JST = timezone(timedelta(hours=9))

# -------------------------
# é€šç”¨å·¥å…·
# -------------------------
def print_cn(msg: str) -> None:
    print(msg)

def ensure_dir(path: str) -> None:
    os.makedirs(path, exist_ok=True)

def safe_get_str(v, default: str = "") -> str:
    if v is None:
        return default
    s = str(v).strip()
    return s if s else default

def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def requests_get_with_retry(url: str) -> Optional[requests.Response]:
    headers = {"User-Agent": "michael-news-bot/1.0"}
    attempts = REQUEST_RETRY_TIMES + 1
    for i in range(1, attempts + 1):
        try:
            resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT_SECONDS)
            resp.raise_for_status()
            return resp
        except requests.exceptions.RequestException as e:
            if i < attempts:
                print_cn(f"âš ï¸ æŠ“å–å¤±è´¥ï¼ˆç¬¬ {i}/{attempts} æ¬¡ï¼‰ï¼š{e}")
                print_cn(f"   {REQUEST_RETRY_SLEEP_SECONDS} ç§’åé‡è¯•...")
                time.sleep(REQUEST_RETRY_SLEEP_SECONDS)
            else:
                print_cn(f"âŒ æŠ“å–å¤±è´¥ï¼ˆå·²é‡è¯• {REQUEST_RETRY_TIMES} æ¬¡ä»å¤±è´¥ï¼‰ï¼š{e}")
                return None
    return None

def parse_datetime_from_entry(entry: dict) -> datetime:
    for key in ("published_parsed", "updated_parsed"):
        parsed = entry.get(key)
        if parsed:
            try:
                ts = time.mktime(parsed)
                return datetime.fromtimestamp(ts, tz=timezone.utc).astimezone(JST)
            except Exception:
                pass
    for key in ("published", "updated"):
        text = entry.get(key)
        if text:
            try:
                dt = date_parser.parse(str(text))
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt.astimezone(JST)
            except Exception:
                pass
    return datetime.now(tz=JST)

def build_item_key(title: str, link: str) -> str:
    return link if link else title

def load_seen() -> Set[str]:
    if not os.path.exists(SEEN_FILE):
        return set()
    try:
        with open(SEEN_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        seen_list = data.get("seen", [])
        return set(str(x) for x in seen_list) if isinstance(seen_list, list) else set()
    except Exception:
        return set()

def save_seen(seen_set: Set[str]) -> None:
    with open(SEEN_FILE, "w", encoding="utf-8") as f:
        json.dump({"seen": sorted(seen_set)}, f, ensure_ascii=False, indent=2)

def load_translation_cache() -> Dict[str, str]:
    if not os.path.exists(TRANSLATION_CACHE_FILE):
        return {}
    try:
        with open(TRANSLATION_CACHE_FILE, "r", encoding="utf-8") as f:
            obj = json.load(f)
        return obj if isinstance(obj, dict) else {}
    except Exception:
        return {}

def save_translation_cache(cache: Dict[str, str]) -> None:
    with open(TRANSLATION_CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

# -------------------------
# æ–‡ç« ç¬¬ä¸€æ®µæå–ï¼ˆNHK/BBC ä¼˜å…ˆè§„åˆ™ï¼‰
# -------------------------
def extract_first_paragraph(url: str, html_text: str) -> str:
    soup = BeautifulSoup(html_text, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    host = urlparse(url).netloc.lower()

    def pick_first_p(container) -> str:
        if not container:
            return ""
        for p in container.find_all("p"):
            t = normalize_text(p.get_text(" ", strip=True))
            # è¿‡æ»¤å¤ªçŸ­ã€åƒå¯¼èˆªçš„æ®µè½
            if len(t) >= 25:
                return t
        return ""

    # NHK
    if "nhk.or.jp" in host:
        candidates = [
            soup.select_one("#js-article-body"),
            soup.select_one("article"),
            soup.select_one("main"),
        ]
        for c in candidates:
            t = pick_first_p(c)
            if t:
                return t

    # BBC
    if "bbc." in host:
        candidates = [
            soup.select_one("article"),
            soup.select_one("main"),
        ]
        for c in candidates:
            t = pick_first_p(c)
            if t:
                return t

    # é€šç”¨å…œåº•ï¼šå…¨ç«™ç¬¬ä¸€ä¸ªå¤Ÿé•¿çš„ p
    for p in soup.find_all("p"):
        t = normalize_text(p.get_text(" ", strip=True))
        if len(t) >= 25:
            return t

    return ""

def fetch_first_paragraph(url: str) -> str:
    if not url:
        return ""
    resp = requests_get_with_retry(url)
    if not resp:
        return ""
    try:
        text = extract_first_paragraph(url, resp.text)
        return text
    except Exception:
        return ""

# -------------------------
# ç¦»çº¿ç¿»è¯‘ï¼ˆArgos Translateï¼‰
# -------------------------
def try_import_argos():
    try:
        import argostranslate.package  # noqa
        import argostranslate.translate  # noqa
        return True
    except Exception:
        return False

def install_argos_models() -> None:
    """
    å®‰è£…ç¦»çº¿æ¨¡å‹ï¼š
    - en->zhï¼ˆBBCï¼‰
    - ja->enï¼ˆNHK èµ°æ—¥->è‹±->ä¸­ï¼‰
    æ³¨æ„ï¼šéœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹ï¼ˆåªåœ¨å®‰è£…æ—¶éœ€è¦ï¼‰
    """
    ok = try_import_argos()
    if not ok:
        print_cn("âŒ ç¦»çº¿ç¿»è¯‘æ¨¡å—å¯¼å…¥å¤±è´¥ã€‚è¯·å…ˆï¼špython -m pip install argostranslate")
        sys.exit(1)

    import argostranslate.package as pkg

    print_cn("ğŸŒ æ­£åœ¨æ›´æ–° Argos æ¨¡å‹ç´¢å¼•ï¼ˆéœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹ï¼‰...")
    pkg.update_package_index()
    available = pkg.get_available_packages()

    need = [("en", "zh"), ("ja", "en")]
    for f, t in need:
        found = None
        for p in available:
            if p.from_code == f and p.to_code == t:
                found = p
                break
        if not found:
            print_cn(f"âš ï¸ æœªåœ¨ç´¢å¼•ä¸­æ‰¾åˆ°ï¼š{f}->{t}")
            continue
        print_cn(f"â¬‡ï¸ å‘ç°æ¨¡å‹ {f}->{t}ï¼Œå¼€å§‹ä¸‹è½½å¹¶å®‰è£…...")
        path = found.download()
        pkg.install_from_path(path)
        print_cn(f"âœ… å·²å®‰è£…ï¼š{f}->{t}")

    print_cn("âœ… æ¨¡å‹å®‰è£…æµç¨‹ç»“æŸã€‚")

def translate_text_offline(text: str, from_code: str, to_code: str,
                           cache: Dict[str, str]) -> str:
    text = safe_get_str(text, "")
    if not text:
        return ""
    key = f"{from_code}->{to_code}:{text}"
    if key in cache:
        return cache[key]

    import argostranslate.translate as tr

    try:
        translated = tr.translate(text, from_code, to_code)
        translated = normalize_text(translated)
        cache[key] = translated
        return translated
    except Exception:
        return ""

def translate_to_zh(original: str, lang: str, cache: Dict[str, str]) -> str:
    """
    lang = 'en' or 'ja'
    - en: en->zh
    - ja: ja->en->zh
    """
    if not original:
        return ""
    if lang == "en":
        return translate_text_offline(original, "en", "zh", cache)
    if lang == "ja":
        mid = translate_text_offline(original, "ja", "en", cache)
        if not mid:
            return ""
        return translate_text_offline(mid, "en", "zh", cache)
    return ""

# -------------------------
# RSS æŠ“å– & åˆå¹¶å»é‡
# -------------------------
def fetch_and_parse_one_feed(feed_name: str, feed_url: str) -> List[Dict]:
    print_cn(f"ğŸ“° æ­£åœ¨æŠ“å– {feed_name}ï¼š{feed_url}")
    resp = requests_get_with_retry(feed_url)
    if not resp:
        print_cn(f"âŒ è·³è¿‡ {feed_name}ï¼ˆæŠ“å–å¤±è´¥ï¼‰")
        return []

    parsed = feedparser.parse(resp.content)
    entries = parsed.get("entries", [])
    print_cn(f"âœ… {feed_name} æŠ“å–æˆåŠŸï¼Œè§£æåˆ° {len(entries)} æ¡æ¡ç›®")

    items: List[Dict] = []
    for entry in entries:
        title = safe_get_str(entry.get("title"), "(æ— æ ‡é¢˜)")
        link = safe_get_str(entry.get("link"), "")
        dt = parse_datetime_from_entry(entry)

        item_key = build_item_key(title, link)
        items.append({
            "title": title,
            "link": link,
            "published": dt.strftime("%Y-%m-%d %H:%M:%S%z"),
            "_published_ts": dt.timestamp(),
            "source": feed_name,
            "_key": item_key,
        })
    return items

def merge_sort_dedupe(items: List[Dict]) -> List[Dict]:
    items_sorted = sorted(items, key=lambda x: x.get("_published_ts", 0), reverse=True)
    seen: Set[str] = set()
    out: List[Dict] = []
    for it in items_sorted:
        k = safe_get_str(it.get("_key"), "")
        if not k or k in seen:
            continue
        seen.add(k)
        out.append(it)
    return out

def filter_new_items(items: List[Dict], seen_before: Set[str], mode_new: bool) -> Tuple[List[Dict], Set[str]]:
    updated = set(seen_before)
    if not mode_new:
        for it in items:
            updated.add(it["_key"])
        return items, updated

    new_items: List[Dict] = []
    for it in items:
        k = it["_key"]
        if k not in seen_before:
            new_items.append(it)
        updated.add(k)
    return new_items, updated

# -------------------------
# ç”Ÿæˆæç®€ç½‘é¡µï¼ˆMichael Newsï¼‰
# -------------------------
def build_site_html(items: List[Dict]) -> str:
    now = datetime.now(tz=JST).strftime("%Y-%m-%d %H:%M:%S %z")

    def esc(s: str) -> str:
        return html.escape(s or "", quote=False)

    rows = []
    for it in items:
        title = esc(it.get("title", ""))
        title_zh = esc(it.get("title_zh", ""))
        para = esc(it.get("summary", ""))
        para_zh = esc(it.get("summary_zh", ""))
        link = esc(it.get("link", ""))
        source = esc(it.get("source", ""))
        published = esc(it.get("published", ""))

        title_line = f'{title}ï¼ˆ{title_zh}ï¼‰' if title_zh else f'{title}ï¼ˆæœªç¿»è¯‘ï¼‰'
        para_line = f'{para}ï¼ˆ{para_zh}ï¼‰' if para_zh else f'{para}ï¼ˆæœªç¿»è¯‘ï¼‰'

        rows.append(f"""
        <div class="card">
          <div class="meta">{source} Â· {published}</div>
          <div class="title">{title_line}</div>
          <div class="para">{para_line}</div>
          <div class="link"><a href="{link}" target="_blank" rel="noopener">æ‰“å¼€åŸæ–‡</a></div>
        </div>
        """)

    body = "\n".join(rows) if rows else '<div class="empty">ä»Šå¤©æ²¡æœ‰æŠ“åˆ°æ–°é—»ã€‚</div>'

    return f"""<!doctype html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Michael News</title>
  <style>
    body{{font-family:system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;
         margin:0;background:#fafafa;color:#111;}}
    .wrap{{max-width:980px;margin:24px auto;padding:0 16px;}}
    h1{{margin:0 0 8px 0;font-size:28px;}}
    .sub{{color:#555;margin-bottom:18px;}}
    .card{{background:#fff;border:1px solid #e7e7e7;border-radius:12px;
          padding:16px;margin:12px 0;box-shadow:0 1px 2px rgba(0,0,0,.03);}}
    .meta{{color:#666;font-size:13px;margin-bottom:10px;}}
    .title{{font-size:18px;font-weight:700;line-height:1.35;margin-bottom:10px;}}
    .para{{font-size:15px;line-height:1.7;color:#222;}}
    .link{{margin-top:10px;font-size:14px;}}
    a{{color:#0b57d0;text-decoration:none;}}
    a:hover{{text-decoration:underline;}}
    .empty{{padding:24px;background:#fff;border:1px dashed #ccc;border-radius:12px;color:#666;}}
  </style>
</head>
<body>
  <div class="wrap">
    <h1>ğŸ“° Michael News</h1>
    <div class="sub">æœ€åæ›´æ–°ï¼š{esc(now)} ï½œ å…± {len(items)} æ¡</div>
    {body}
  </div>
</body>
</html>"""

def write_site_files(items: List[Dict]) -> None:
    ensure_dir(SITE_DIR)
    # åŒæ—¶è¾“å‡ºä¸€ä¸ª jsonï¼ˆå¯é€‰ï¼Œæ–¹ä¾¿ä½ è°ƒè¯•ï¼‰
    with open(os.path.join(SITE_DIR, "news.json"), "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=2)

    html_text = build_site_html(items)
    with open(os.path.join(SITE_DIR, "index.html"), "w", encoding="utf-8") as f:
        f.write(html_text)

# -------------------------
# ä¸»æµç¨‹
# -------------------------
def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()
    g = p.add_mutually_exclusive_group()
    g.add_argument("--new", action="store_true", help="åªè¾“å‡ºæ–°å¢ï¼ˆé»˜è®¤ï¼‰")
    g.add_argument("--all", action="store_true", help="è¾“å‡ºå…¨éƒ¨ï¼ˆä¸åšå¢é‡ï¼‰")
    p.add_argument("--limit", type=int, default=DEFAULT_PRINT_LIMIT, help="ç»ˆç«¯æ‰“å°æ¡æ•°")
    p.add_argument("--install-models", action="store_true", help="å®‰è£…ç¦»çº¿ç¿»è¯‘æ¨¡å‹")
    return p.parse_args()

def main() -> None:
    args = parse_args()

    if args.install_models:
        install_argos_models()
        return

    mode_new = not args.all
    seen_before = load_seen()

    all_items: List[Dict] = []
    for feed in RSS_FEEDS:
        all_items.extend(fetch_and_parse_one_feed(feed["name"], feed["url"]))

    if not all_items:
        print_cn("âŒ æ²¡æŠ“åˆ°ä»»ä½•æ¡ç›®ã€‚")
        return

    merged = merge_sort_dedupe(all_items)
    print_cn(f"ğŸ” åˆå¹¶åå»é‡ï¼š{len(merged)} æ¡ï¼ˆæ¥è‡ª {len(RSS_FEEDS)} ä¸ªæºï¼‰")

    selected, updated_seen = filter_new_items(merged, seen_before, mode_new)
    if mode_new:
        print_cn(f"ğŸ†• æ–°å¢æ–°é—»ï¼š{len(selected)} æ¡ï¼ˆé»˜è®¤åªè¾“å‡ºæ–°å¢ï¼‰")
    else:
        print_cn(f"ğŸ“¦ è¾“å‡ºå…¨éƒ¨ï¼š{len(selected)} æ¡ï¼ˆä¸åšå¢é‡ï¼‰")

    # æŠ“ç¬¬ä¸€æ®µ
    if selected:
        print_cn(f"ğŸ§¾ æ­£åœ¨ä¸ºæœ¬æ¬¡è¾“å‡ºçš„ {len(selected)} æ¡æ–°é—»æŠ“å–â€œç¬¬ä¸€æ®µæ‘˜è¦â€...")
        for i, it in enumerate(selected, start=1):
            link = it.get("link", "")
            if not link:
                it["summary"] = ""
            else:
                print_cn(f"   [{i}/{len(selected)}] æŠ“æ‘˜è¦ï¼š{link}")
                it["summary"] = fetch_first_paragraph(link)
            time.sleep(ARTICLE_FETCH_SLEEP_SECONDS)

    # ç¦»çº¿ç¿»è¯‘ï¼ˆå¦‚æœ argos æ²¡è£…ï¼Œå°±è·³è¿‡ï¼‰
    cache = load_translation_cache()
    if try_import_argos():
        print_cn("ğŸŒ æ­£åœ¨æŠŠæ ‡é¢˜ä¸æ‘˜è¦ç¦»çº¿ç¿»è¯‘æˆä¸­æ–‡ï¼ˆæ—  Keyï¼‰...")
        for i, it in enumerate(selected, start=1):
            source = it.get("source", "")
            lang = "ja" if "NHK" in source else "en"
            t = it.get("title", "")
            s = it.get("summary", "")
            if t:
                it["title_zh"] = translate_to_zh(t, lang, cache)
            else:
                it["title_zh"] = ""
            if s:
                it["summary_zh"] = translate_to_zh(s, lang, cache)
            else:
                it["summary_zh"] = ""
            if i % 10 == 0:
                save_translation_cache(cache)
        save_translation_cache(cache)
    else:
        print_cn("âš ï¸ æœªæ£€æµ‹åˆ° argostranslateï¼Œè·³è¿‡ç¿»è¯‘ã€‚ï¼ˆä½ å¯ä»¥è¿è¡Œï¼špython fetch_news.py --install-modelsï¼‰")
        for it in selected:
            it["title_zh"] = ""
            it["summary_zh"] = ""

    # ä¿å­˜ outputï¼ˆå¯é€‰ï¼‰
    ensure_dir(OUTPUT_DIR)
    out_path = os.path.join(OUTPUT_DIR, f"news_{datetime.now(tz=JST).strftime('%Y%m%d_%H%M%S')}.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(selected, f, ensure_ascii=False, indent=2)
    print_cn(f"ğŸ’¾ å·²ä¿å­˜åˆ°ï¼š{out_path}")

    # ç”Ÿæˆç«™ç‚¹
    write_site_files(selected)
    print_cn(f"âœ… å·²ç”Ÿæˆç«™ç‚¹ï¼š{SITE_DIR}/index.html")

    save_seen(updated_seen)

    # ç»ˆç«¯ç®€ç•¥æ‰“å°
    limit = max(1, int(args.limit))
    print_cn(f"\nğŸ“Œ ç»ˆç«¯å±•ç¤ºæœ€æ–° {min(limit, len(selected))} æ¡ï¼š")
    print_cn("-" * 60)
    for idx, it in enumerate(selected[:limit], start=1):
        print_cn(f"{idx}. [{it.get('published','')}] ({it.get('source','')})")
        print_cn(f"   æ ‡é¢˜ï¼š{it.get('title','')}")
        print_cn(f"   æ ‡é¢˜ï¼ˆä¸­æ–‡ï¼‰ï¼š{it.get('title_zh','(æœªç¿»è¯‘)')}")
        print_cn(f"   æ‘˜è¦ï¼š{it.get('summary','')}")
        print_cn(f"   æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰ï¼š{it.get('summary_zh','(æœªç¿»è¯‘)')}")
        print_cn(f"   é“¾æ¥ï¼š{it.get('link','')}\n")
    print_cn("-" * 60)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print_cn("\nğŸ›‘ ä½ æ‰‹åŠ¨ä¸­æ–­äº†ç¨‹åºï¼ˆCtrl+Cï¼‰")
        sys.exit(0)


